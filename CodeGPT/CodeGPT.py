#!/usr/bin/env python
# coding: utf-8

# In[ ]:


# !pip install datasets==2.14.5
# !pip install transformers==4.34.1
# !pip install scikit-learn==1.3.1
# !pip install tqdm==4.66.1
# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
# !pip install evaluate==0.4.1
# !pip install nltk==3.8.1
# !pip install accelerate==0.23.0


# In[1]:


import pandas as pd
from datasets import load_from_disk, Dataset

def load_dataset() -> Dataset:
    """ Load dataset. """
    dataset = load_from_disk("../datasets/diverse_vul_minified_balanced_5050.hf")
    dataset = dataset.shuffle(seed=42)

    return dataset


# In[2]:


dataset = load_dataset()


# In[4]:


from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained("microsoft/CodeGPT-small-py")
model = AutoModelForSequenceClassification.from_pretrained("microsoft/CodeGPT-small-py", num_labels=2)
tokenizer.pad_token = tokenizer.eos_token


# In[5]:


def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=512)

tokenizer.pad_token = tokenizer.eos_token
tokenized_datasets = dataset.map(tokenize_function, batched=True)


# In[6]:


import evaluate
import numpy as np

metric = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)


# In[7]:


from transformers import Trainer, TrainingArguments
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Define the training arguments
training_args = TrainingArguments(
    output_dir="./codegpt-minified_diversevul_balanced_5050",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    num_train_epochs=10,
    warmup_steps=1000,
    greater_is_better=True,
    load_best_model_at_end=True
)

# Define a Trainer instance with the prepared datasets
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    compute_metrics=compute_metrics
)


print("### STARTING TRAINING ###")
trainer.train()

trainer.save_model("best_codeGPT_model_minified_diverse_vul")


# In[8]:


from tqdm.auto import tqdm
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from sklearn.metrics import classification_report

def evaluate() -> None:
    """Evaluate the model on the test dataset."""
    predictions_list, labels_list = [], []

    # Make predictions on the test set
    test_results = trainer.predict(tokenized_datasets["test"])
    predictions_list = test_results.predictions.argmax(axis=1)

    predicted_labels = test_results.predictions.argmax(axis=1)
    labels_list = tokenized_datasets["test"]["label"]

    report = classification_report(labels_list, predictions_list)
    print(report)


# In[ ]:


evaluate()

