{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (2.2.1+cu121)\n",
      "Requirement already satisfied: torchvision in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (0.17.1+cu121)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (2.2.1+cu121)\n",
      "Requirement already satisfied: filelock in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR=\"Results/CodeGPT_Original_wduplicates_CVE_Fixes_5050_opt_lora_batch_16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m1180939\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_LOG_MODEL=true\n",
      "env: WANDB_NOTEBOOK_NAME=DiverseVul_CodeGPT\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "%env WANDB_LOG_MODEL=true\n",
    "%env WANDB_NOTEBOOK_NAME=DiverseVul_CodeGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_from_disk, Dataset\n",
    "\n",
    "def load_dataset() -> Dataset:\n",
    "    \"\"\" Load dataset. \"\"\"\n",
    "    dataset = load_from_disk(\"../datasets/cve_fixes_original_balanced_5050_wduplicates.hf/\")\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 7003\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2002\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1001\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at microsoft/CodeGPT-small-py and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/CodeGPT-small-py\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/CodeGPT-small-py\", num_labels=2)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 17176\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, concatenate_datasets\n",
    "\n",
    "\n",
    "# The maximum total input sequence length after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"], dataset[\"validation\"]]).map(\n",
    "    lambda x: tokenizer(x[\"text\"], truncation=True), batched=True, remove_columns=['text', 'label']\n",
    ")\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "max_source_length=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7003/7003 [00:01<00:00, 3595.21 examples/s]\n",
      "Map: 100%|██████████| 2002/2002 [00:00<00:00, 3693.60 examples/s]\n",
      "Map: 100%|██████████| 1001/1001 [00:00<00:00, 3253.42 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=False, truncation=True, max_length=max_source_length)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure LoRA (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\peft\\tuners\\lora\\layer.py:1059: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2ForSequenceClassification(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50001, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): lora.Linear(\n",
       "                (base_layer): Conv1D()\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (c_proj): Conv1D()\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=2, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=2, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "lora_config = LoraConfig(inference_mode=True, r=8,\n",
    "lora_alpha=32,\n",
    "lora_dropout=0.05,\n",
    "task_type=TaskType.SEQ_CLS)\n",
    "\n",
    "from peft import get_peft_model\n",
    "model_peft = get_peft_model(model, lora_config)\n",
    "model_peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### STARTING TRAINING ###\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\tiago\\OneDrive - Instituto Superior de Engenharia do Porto\\Documents\\GECAD\\DiverseVul LLMs Analysis\\DiverseVul Code\\CodeGPT\\wandb\\run-20240407_191726-9mk4qz2c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/1180939/huggingface/runs/9mk4qz2c' target=\"_blank\">CodeGPT_Original_wduplicates_CVE_Fixes_5050_opt_lora_batch_16</a></strong> to <a href='https://wandb.ai/1180939/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/1180939/huggingface' target=\"_blank\">https://wandb.ai/1180939/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/1180939/huggingface/runs/9mk4qz2c' target=\"_blank\">https://wandb.ai/1180939/huggingface/runs/9mk4qz2c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 200/4380 [02:54<1:02:10,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1566, 'grad_norm': 6.839037895202637, 'learning_rate': 2e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  5%|▍         | 200/4380 [03:41<1:02:10,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7751638293266296, 'eval_f1': 0.65045170257123, 'eval_runtime': 47.3698, 'eval_samples_per_second': 21.132, 'eval_steps_per_second': 2.66, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 400/4380 [06:41<1:01:27,  1.08it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7263, 'grad_norm': 4.598362922668457, 'learning_rate': 1.904306220095694e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  9%|▉         | 400/4380 [07:33<1:01:27,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7242880463600159, 'eval_f1': 0.4598459845984598, 'eval_runtime': 51.8726, 'eval_samples_per_second': 19.297, 'eval_steps_per_second': 2.429, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 600/4380 [10:42<47:59,  1.31it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.717, 'grad_norm': 4.9303131103515625, 'learning_rate': 1.8086124401913876e-05, 'epoch': 1.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 14%|█▎        | 600/4380 [11:35<47:59,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7221130728721619, 'eval_f1': 0.4837696335078534, 'eval_runtime': 53.4647, 'eval_samples_per_second': 18.723, 'eval_steps_per_second': 2.357, 'epoch': 1.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 800/4380 [14:55<1:03:37,  1.07s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7136, 'grad_norm': 4.3646979331970215, 'learning_rate': 1.7129186602870815e-05, 'epoch': 1.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 18%|█▊        | 800/4380 [15:52<1:03:37,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7214221358299255, 'eval_f1': 0.5133795837462835, 'eval_runtime': 57.782, 'eval_samples_per_second': 17.324, 'eval_steps_per_second': 2.181, 'epoch': 1.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 1000/4380 [19:18<59:37,  1.06s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7155, 'grad_norm': 6.074002265930176, 'learning_rate': 1.6172248803827754e-05, 'epoch': 2.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 23%|██▎       | 1000/4380 [20:17<59:37,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7197231650352478, 'eval_f1': 0.46616541353383456, 'eval_runtime': 59.1916, 'eval_samples_per_second': 16.911, 'eval_steps_per_second': 2.129, 'epoch': 2.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 1200/4380 [23:50<55:57,  1.06s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7154, 'grad_norm': 3.678102731704712, 'learning_rate': 1.5215311004784689e-05, 'epoch': 2.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 27%|██▋       | 1200/4380 [24:46<55:57,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7190898656845093, 'eval_f1': 0.5247148288973384, 'eval_runtime': 56.1127, 'eval_samples_per_second': 17.839, 'eval_steps_per_second': 2.245, 'epoch': 2.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 1400/4380 [28:11<52:12,  1.05s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.719, 'grad_norm': 3.2395880222320557, 'learning_rate': 1.4258373205741626e-05, 'epoch': 3.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 32%|███▏      | 1400/4380 [29:07<52:12,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7188019752502441, 'eval_f1': 0.4834710743801653, 'eval_runtime': 56.518, 'eval_samples_per_second': 17.711, 'eval_steps_per_second': 2.229, 'epoch': 3.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 1600/4380 [32:26<42:06,  1.10it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7107, 'grad_norm': 3.590299367904663, 'learning_rate': 1.3301435406698567e-05, 'epoch': 3.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 37%|███▋      | 1600/4380 [33:20<42:06,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.717654287815094, 'eval_f1': 0.5220729366602687, 'eval_runtime': 54.1411, 'eval_samples_per_second': 18.489, 'eval_steps_per_second': 2.327, 'epoch': 3.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 1800/4380 [36:37<39:15,  1.10it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7148, 'grad_norm': 16.197399139404297, 'learning_rate': 1.2344497607655504e-05, 'epoch': 4.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 41%|████      | 1800/4380 [37:31<39:15,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7173829674720764, 'eval_f1': 0.515625, 'eval_runtime': 54.083, 'eval_samples_per_second': 18.509, 'eval_steps_per_second': 2.33, 'epoch': 4.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 2000/4380 [40:47<32:58,  1.20it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.712, 'grad_norm': 5.092569351196289, 'learning_rate': 1.1387559808612441e-05, 'epoch': 4.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 46%|████▌     | 2000/4380 [41:41<32:58,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7170596718788147, 'eval_f1': 0.5098425196850394, 'eval_runtime': 54.0429, 'eval_samples_per_second': 18.522, 'eval_steps_per_second': 2.331, 'epoch': 4.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2200/4380 [44:59<36:29,  1.00s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7154, 'grad_norm': 5.661418914794922, 'learning_rate': 1.0430622009569378e-05, 'epoch': 5.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 50%|█████     | 2200/4380 [45:53<36:29,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7165525555610657, 'eval_f1': 0.49390243902439024, 'eval_runtime': 54.1153, 'eval_samples_per_second': 18.498, 'eval_steps_per_second': 2.328, 'epoch': 5.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 2400/4380 [49:12<34:11,  1.04s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7099, 'grad_norm': 12.033342361450195, 'learning_rate': 9.473684210526315e-06, 'epoch': 5.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 55%|█████▍    | 2400/4380 [50:07<34:11,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7161293625831604, 'eval_f1': 0.500998003992016, 'eval_runtime': 55.7724, 'eval_samples_per_second': 17.948, 'eval_steps_per_second': 2.259, 'epoch': 5.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 2600/4380 [53:31<28:59,  1.02it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7162, 'grad_norm': 4.571951389312744, 'learning_rate': 8.516746411483254e-06, 'epoch': 5.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 59%|█████▉    | 2600/4380 [54:27<28:59,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7158545851707458, 'eval_f1': 0.5193050193050193, 'eval_runtime': 55.6812, 'eval_samples_per_second': 17.977, 'eval_steps_per_second': 2.263, 'epoch': 5.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 2800/4380 [57:52<25:00,  1.05it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.714, 'grad_norm': 5.1120100021362305, 'learning_rate': 7.5598086124401915e-06, 'epoch': 6.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 64%|██████▍   | 2800/4380 [58:47<25:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.715874195098877, 'eval_f1': 0.5394853593611357, 'eval_runtime': 55.6401, 'eval_samples_per_second': 17.991, 'eval_steps_per_second': 2.265, 'epoch': 6.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 3000/4380 [1:02:11<24:33,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7105, 'grad_norm': 5.303255558013916, 'learning_rate': 6.6028708133971295e-06, 'epoch': 6.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 68%|██████▊   | 3000/4380 [1:03:07<24:33,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.715654730796814, 'eval_f1': 0.5306495882891126, 'eval_runtime': 55.7972, 'eval_samples_per_second': 17.94, 'eval_steps_per_second': 2.258, 'epoch': 6.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 3200/4380 [1:06:32<22:14,  1.13s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7096, 'grad_norm': 5.1444196701049805, 'learning_rate': 5.645933014354067e-06, 'epoch': 7.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 73%|███████▎  | 3200/4380 [1:07:31<22:14,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.715352475643158, 'eval_f1': 0.5172413793103449, 'eval_runtime': 59.0194, 'eval_samples_per_second': 16.961, 'eval_steps_per_second': 2.135, 'epoch': 7.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 3400/4380 [1:11:10<16:48,  1.03s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7086, 'grad_norm': 5.2123308181762695, 'learning_rate': 4.6889952153110055e-06, 'epoch': 7.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 78%|███████▊  | 3400/4380 [1:12:07<16:48,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7152208089828491, 'eval_f1': 0.49126413155190135, 'eval_runtime': 57.1645, 'eval_samples_per_second': 17.511, 'eval_steps_per_second': 2.204, 'epoch': 7.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 3600/4380 [1:15:28<13:03,  1.00s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7124, 'grad_norm': 6.04718017578125, 'learning_rate': 3.732057416267943e-06, 'epoch': 8.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 82%|████████▏ | 3600/4380 [1:16:24<13:03,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7151667475700378, 'eval_f1': 0.50199203187251, 'eval_runtime': 55.6068, 'eval_samples_per_second': 18.001, 'eval_steps_per_second': 2.266, 'epoch': 8.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 3800/4380 [1:19:47<10:06,  1.05s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7147, 'grad_norm': 7.728768348693848, 'learning_rate': 2.7751196172248807e-06, 'epoch': 8.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 87%|████████▋ | 3800/4380 [1:20:41<10:06,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7151534557342529, 'eval_f1': 0.498989898989899, 'eval_runtime': 54.6836, 'eval_samples_per_second': 18.305, 'eval_steps_per_second': 2.304, 'epoch': 8.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 4000/4380 [1:24:00<06:27,  1.02s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7021, 'grad_norm': 4.038424968719482, 'learning_rate': 1.8181818181818183e-06, 'epoch': 9.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 91%|█████████▏| 4000/4380 [1:24:54<06:27,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7151246666908264, 'eval_f1': 0.532608695652174, 'eval_runtime': 54.2171, 'eval_samples_per_second': 18.463, 'eval_steps_per_second': 2.324, 'epoch': 9.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 4200/4380 [1:28:12<03:06,  1.04s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7121, 'grad_norm': 11.030218124389648, 'learning_rate': 8.612440191387561e-07, 'epoch': 9.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 96%|█████████▌| 4200/4380 [1:29:06<03:06,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7153042554855347, 'eval_f1': 0.5411140583554377, 'eval_runtime': 54.1594, 'eval_samples_per_second': 18.482, 'eval_steps_per_second': 2.326, 'epoch': 9.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4380/4380 [1:32:05<00:00,  1.10it/s]c:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 5531.2808, 'train_samples_per_second': 12.661, 'train_steps_per_second': 0.792, 'train_loss': 0.7334746704798311, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4380/4380 [1:32:07<00:00,  1.26s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4380, training_loss=0.7334746704798311, metrics={'train_runtime': 5531.2808, 'train_samples_per_second': 12.661, 'train_steps_per_second': 0.792, 'train_loss': 0.7334746704798311, 'epoch': 10.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./CodeGPT_Original_wduplicates_CVEFixes_5050_lora_batch_16\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    warmup_steps=200,\n",
    "    logging_steps=200,\n",
    "    save_steps=200,\n",
    "    eval_steps=200,\n",
    "    logging_dir=RESULTS_DIR + \"/logs\",  # Replace with your desired logging directory\n",
    "    report_to=\"wandb\",\n",
    "    run_name = RESULTS_DIR.split('/')[1],\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Define a Trainer instance with the prepared datasets\n",
    "trainer = Trainer(\n",
    "    model=model_peft,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "print(\"### STARTING TRAINING ###\")\n",
    "trainer.train()\n",
    "\n",
    "# trainer.save_model(\"best_codeGPT_model_minified_diverse_vul_learning_rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [01:25<00:00,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.38      0.42      1001\n",
      "           1       0.49      0.59      0.53      1001\n",
      "\n",
      "    accuracy                           0.48      2002\n",
      "   macro avg       0.48      0.48      0.48      2002\n",
      "weighted avg       0.48      0.48      0.48      2002\n",
      "\n",
      "0.484015984015984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from scipy import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def evaluate(model) -> None:\n",
    "    \"\"\"Evaluate the model on the test dataset.\"\"\"\n",
    "    predictions_list, labels_list = [], []\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    test_results = model.predict(tokenized_datasets[\"test\"])\n",
    "    predictions_list = test_results.predictions.argmax(axis=1)\n",
    "\n",
    "    labels_list = tokenized_datasets[\"test\"][\"label\"]\n",
    "\n",
    "    report = classification_report(labels_list, predictions_list)\n",
    "    accuracy = accuracy_score(labels_list, predictions_list)\n",
    "    print(report)\n",
    "    print(accuracy)\n",
    "\n",
    "evaluate(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
