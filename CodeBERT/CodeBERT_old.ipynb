{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6981bba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary configurations to read the dataset (there is an issue with the hash field.)\n",
    "\n",
    "import json as simplejson\n",
    "import pandas as pd\n",
    "\n",
    "pd.io.json._json.loads = lambda s, *a, **kw: json.loads(s)\n",
    "\n",
    "# monkeypatch using faster simplejson module\n",
    "pd.io.json._json.loads = lambda s, *a, **kw: simplejson.loads(s)\n",
    "\n",
    "# normalising (unnesting) at the same time (for nested jsons)\n",
    "pd.io.json._json.loads = lambda s, *a, **kw: pd.json_normalize(simplejson.loads(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7916f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import clang\n",
    "\n",
    "diverse_vul = pd.read_json(\"../DiverseVul_Dataset.json\", orient=\"records\", lines=True)\n",
    "diverse_vul[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf41149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_function(code):\n",
    "\n",
    "    # Convert to lowercase\n",
    "#     code = code.lower()\n",
    "\n",
    "    # Remove comments\n",
    "    code = re.sub(r\"\\/\\/.*\", \"\", code)\n",
    "    code = re.sub(r\"\\/\\*[\\s\\S]*?\\*\\/\", \"\", code)\n",
    "    \n",
    "    # Remove whitespace\n",
    "    code = re.sub(r\"\\s+\", \" \", code)\n",
    "    \n",
    "    # Remove semicolons\n",
    "#     code = code.replace(\";\", \"\")\n",
    "    \n",
    "    # Remove curly braces\n",
    "#     code = code.replace(\"{\", \"\")\n",
    "#     code = code.replace(\"}\", \"\")\n",
    "    \n",
    "    return code\n",
    "\n",
    "# Looping to change function values\n",
    "for i, x in enumerate(diverse_vul['func']):\n",
    "    diverse_vul.at[i, 'func'] = clean_function(diverse_vul['func'][i])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45820f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diverse_vul['func'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5819f660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of 'non-vulnerable' occurrences in the 'CWE-Binary' column\n",
    "nonvulnerable = len(diverse_vul[diverse_vul['target'] == 0])\n",
    "print(\"Non-Vulnerable: \", nonvulnerable)\n",
    "\n",
    "# Calculate the 'vulnerable' occurrences in the 'CWE-Binary' column\n",
    "vulnerable = len(diverse_vul) - nonvulnerable\n",
    "print(\"Vulnerable: \", vulnerable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e2e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vulnerable_indices = balanced_dataset[diverse_vul['target'] == 1].sample(0, random_state=42).index\n",
    "non_vulnerable_indices = diverse_vul[diverse_vul['target'] == 0].sample(292602, random_state=42).index\n",
    "\n",
    "# Drop the selected rows from the dataset\n",
    "balanced_dataset = diverse_vul.drop(index=vulnerable_indices.union(non_vulnerable_indices))\n",
    "\n",
    "# Check the distribution of classes in the modified dataset\n",
    "print(balanced_dataset['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0573f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into features (X) and labels (y)\n",
    "X = balanced_dataset['func']\n",
    "y = balanced_dataset['target']\n",
    "\n",
    "# Split the data into train, validation, and test sets (80-10-10 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125, random_state=42, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f937955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_string = max(X, key=len)\n",
    "len(longest_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d07f081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load the CodeBERT tokenizer and model\n",
    "model_name = \"microsoft/codebert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Tokenize the train, validation, and test datasets\n",
    "train_encodings = tokenizer(list(X_train), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(X_test), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(list(X_val), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af84d45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Create a dictionary with keys as tokenized outputs\n",
    "tokenized_train_dict = {\n",
    "    \"input_ids\": train_encodings[\"input_ids\"],\n",
    "    \"attention_mask\": train_encodings[\"attention_mask\"],\n",
    "    \"labels\": y_train\n",
    "}\n",
    "\n",
    "tokenized_test_dict = {\n",
    "    \"input_ids\": test_encodings[\"input_ids\"],\n",
    "    \"attention_mask\": test_encodings[\"attention_mask\"],\n",
    "    \"labels\": y_test\n",
    "}\n",
    "\n",
    "tokenized_val_dict = {\n",
    "    \"input_ids\": val_encodings[\"input_ids\"],\n",
    "    \"attention_mask\": val_encodings[\"attention_mask\"],\n",
    "    \"labels\": y_val\n",
    "}\n",
    "\n",
    "# Convert tokenized outputs to a Dataset\n",
    "train_dataset = Dataset.from_dict(tokenized_train_dict)\n",
    "test_dataset = Dataset.from_dict(tokenized_test_dict)\n",
    "val_dataset = Dataset.from_dict(tokenized_val_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5c2bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c46982",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, r=16, lora_alpha=16, lora_dropout=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe6c97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7af0896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model\n",
    "model_peft = get_peft_model(model, lora_config)\n",
    "model_peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c77e814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e3180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_peft.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b7760a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import evaluate\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results-balanced-1000-samples\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=10,\n",
    "    warmup_steps=1000,\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "metric = evaluate.load(\"f1\")\n",
    "\n",
    "# Define a Trainer instance with the prepared datasets\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"best_codebert_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a17c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "test_results = trainer.predict(test_dataset)\n",
    "predicted_labels = test_results.predictions.argmax(axis=1)\n",
    "\n",
    "predicted_labels = test_results.predictions.argmax(axis=1)\n",
    "true_labels = y_test\n",
    "\n",
    "# Compute performance metrics on the test set\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Test set metrics:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e38bdacfe1d6e2b7e20a64d2eef782d7715e4314dc541be72f500d3c69b94a4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
