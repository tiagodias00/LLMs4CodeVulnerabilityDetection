{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcb00709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.16.4-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (from wandb) (8.1.7)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Using cached GitPython-3.1.42-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (from wandb) (5.9.0)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-1.42.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (from wandb) (6.0.1)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.3-cp310-cp310-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (from wandb) (68.2.2)\n",
      "Collecting appdirs>=1.4.3 (from wandb)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting protobuf!=4.21.0,<5,>=3.19.0 (from wandb)\n",
      "  Using cached protobuf-4.25.3-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: colorama in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (from Click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading wandb-0.16.4-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.2 MB 7.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.8/2.2 MB 10.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.5/2.2 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 12.7 MB/s eta 0:00:00\n",
      "Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Using cached GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
      "Using cached protobuf-4.25.3-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Downloading sentry_sdk-1.42.0-py2.py3-none-any.whl (263 kB)\n",
      "   ---------------------------------------- 0.0/263.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 263.5/263.5 kB 16.9 MB/s eta 0:00:00\n",
      "Downloading setproctitle-1.3.3-cp310-cp310-win_amd64.whl (11 kB)\n",
      "Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: appdirs, smmap, setproctitle, sentry-sdk, protobuf, docker-pycreds, gitdb, GitPython, wandb\n",
      "Successfully installed GitPython-3.1.42 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.11 protobuf-4.25.3 sentry-sdk-1.42.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.4\n"
     ]
    }
   ],
   "source": [
    "# !pip install git+https://github.com/huggingface/transformers@main\n",
    "# !pip install datasets\n",
    "# !pip install scikit-learn\n",
    "# !pip install tqdm\n",
    "# !pip install evaluate\n",
    "# !pip install nltk\n",
    "# !pip install accelerate\n",
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install https://huggingface.co/r4ziel/xformers_pre_built/resolve/main/triton-2.0.0-cp310-cp310-win_amd64.whl\n",
    "# !pip install mamba-ssm\n",
    "! pip install wandb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8b9021",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35f3af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR=\"Results/Mamba_Minified_CVE_Fixes_5050_batch_16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3f92c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m1180939\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_LOG_MODEL=true\n",
      "env: WANDB_NOTEBOOK_NAME=DiverseVul_Mamba\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "%env WANDB_LOG_MODEL=true\n",
    "%env WANDB_NOTEBOOK_NAME=DiverseVul_Mamba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99811f76",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7916f206",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_from_disk, Dataset\n",
    "\n",
    "def load_dataset() -> Dataset:\n",
    "    \"\"\" Load dataset. \"\"\"\n",
    "    dataset = load_from_disk(\"../datasets/cve_fixes_minified_balanced_5050.hf/\")\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bf41149",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af0d3621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting causal-conv1d\n",
      "  Using cached causal_conv1d-1.2.0.post2.tar.gz (7.1 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: torch in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (from causal-conv1d) (2.2.1+cu121)\n",
      "Requirement already satisfied: packaging in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (from causal-conv1d) (24.0)\n",
      "Collecting ninja (from causal-conv1d)\n",
      "  Using cached ninja-1.11.1.1-py2.py3-none-win_amd64.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (from torch->causal-conv1d) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (from torch->causal-conv1d) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (from torch->causal-conv1d) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (from torch->causal-conv1d) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (from torch->causal-conv1d) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (from torch->causal-conv1d) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (from jinja2->torch->causal-conv1d) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\tiago\\anaconda3\\envs\\mamba_env\\lib\\site-packages (from sympy->torch->causal-conv1d) (1.3.0)\n",
      "Using cached ninja-1.11.1.1-py2.py3-none-win_amd64.whl (312 kB)\n",
      "Building wheels for collected packages: causal-conv1d\n",
      "  Building wheel for causal-conv1d (setup.py): started\n",
      "  Building wheel for causal-conv1d (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for causal-conv1d\n",
      "Failed to build causal-conv1d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py bdist_wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [31 lines of output]\n",
      "      \n",
      "      \n",
      "      torch.__version__  = 2.2.1+cu121\n",
      "      \n",
      "      \n",
      "      running bdist_wheel\n",
      "      Guessing wheel URL:  https://github.com/Dao-AILab/causal-conv1d/releases/download/v1.2.0.post2/causal_conv1d-1.2.0.post2+cu122torch2.2cxx11abiFALSE-cp310-cp310-win_amd64.whl\n",
      "      Precompiled wheel not found. Building from source...\n",
      "      C:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\torch\\utils\\cpp_extension.py:500: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
      "        warnings.warn(msg.format('we could not find ninja.'))\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-310\n",
      "      creating build\\lib.win-amd64-cpython-310\\causal_conv1d\n",
      "      copying causal_conv1d\\causal_conv1d_interface.py -> build\\lib.win-amd64-cpython-310\\causal_conv1d\n",
      "      copying causal_conv1d\\__init__.py -> build\\lib.win-amd64-cpython-310\\causal_conv1d\n",
      "      running build_ext\n",
      "      C:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\torch\\utils\\cpp_extension.py:381: UserWarning: Error checking compiler version for cl: [WinError 2] O sistema não conseguiu localizar o ficheiro especificado\n",
      "        warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n",
      "      C:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\torch\\utils\\cpp_extension.py:415: UserWarning: The detected CUDA version (12.4) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.\n",
      "        warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
      "      building 'causal_conv1d_cuda' extension\n",
      "      creating build\\temp.win-amd64-cpython-310\n",
      "      creating build\\temp.win-amd64-cpython-310\\Release\n",
      "      creating build\\temp.win-amd64-cpython-310\\Release\\csrc\n",
      "      \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.39.33519\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -IC:\\Users\\tiago\\AppData\\Local\\Temp\\pip-install-yt9azqho\\causal-conv1d_12f8f426564e4005825ade36a06cdc3f\\csrc\\causal_conv1d -IC:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\torch\\include -IC:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\torch\\include\\torch\\csrc\\api\\include -IC:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\torch\\include\\TH -IC:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\torch\\include\\THC \"-IC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.4\\include\" -IC:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\include -IC:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\Include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.39.33519\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Auxiliary\\VS\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.22621.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\cppwinrt\" /EHsc /Tpcsrc/causal_conv1d.cpp /Fobuild\\temp.win-amd64-cpython-310\\Release\\csrc/causal_conv1d.obj /MD /wd4819 /wd4251 /wd4244 /wd4267 /wd4275 /wd4018 /wd4190 /wd4624 /wd4067 /wd4068 /EHsc -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=causal_conv1d_cuda -D_GLIBCXX_USE_CXX11_ABI=0 /std:c++17\n",
      "      cl : Linha de comando warning D9002 : ignorando op\\x87\\xc6o desconhecida '-O3'\n",
      "      causal_conv1d.cpp\n",
      "      c1xx: fatal error C1083: N\\xc6o \\x82 poss\\xa1vel abrir arquivo fonte: 'csrc/causal_conv1d.cpp': No such file or directory\n",
      "      error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2022\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.39.33519\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 2\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for causal-conv1d\n",
      "ERROR: Could not build wheels for causal-conv1d, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "!pip install causal-conv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74fc15aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mamba_ssm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, MambaForCausalLM\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmamba_ssm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixer_seq_simple\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MambaLMHeadModel\n\u001b[0;32m      4\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate-spaces/mamba-790m-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mamba_ssm'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, MambaForCausalLM\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "\n",
    "model_name = \"state-spaces/mamba-790m-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = MambaLMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5822cd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6456/6456 [00:00<00:00, 21017.88 examples/s]\n",
      "Map: 100%|██████████| 1845/1845 [00:00<00:00, 15818.83 examples/s]\n",
      "Map: 100%|██████████| 923/923 [00:00<00:00, 17815.74 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'nonvulnerable', 'vulnerable', 'vulnerable', 'nonvulnerable', 'vulnerable']\n"
     ]
    }
   ],
   "source": [
    "# Define a mapping from numerical labels to string labels\n",
    "label_mapping = {0: \"nonvulnerable\", 1: \"vulnerable\"}\n",
    "\n",
    "# Define a function to map numerical labels to string labels\n",
    "def map_labels(example):\n",
    "    example[\"label\"] = label_mapping[example[\"label\"]]\n",
    "    return example\n",
    "\n",
    "# Apply the label mapping function to the train, test, and validation splits\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].map(map_labels)\n",
    "\n",
    "# Display the modified dataset\n",
    "print(dataset[\"train\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7907272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 17781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8301/8301 [00:00<00:00, 122305.81 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max target length: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "# Metric\n",
    "metric = evaluate.load(\"f1\")\n",
    "\n",
    "# The maximum total input sequence length after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(\n",
    "    lambda x: tokenizer(x[\"text\"], truncation=True), batched=True, remove_columns=['text', 'label']\n",
    ")\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(\n",
    "    lambda x: tokenizer(x[\"label\"], truncation=True), batched=True, remove_columns=['text', 'label']\n",
    ")\n",
    "max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n",
    "print(f\"Max target length: {max_target_length}\")\n",
    "\n",
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./Mamba_Minified_Diverse_Vul_5050_lora_batch_16_ERASE\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=10,\n",
    "    warmup_steps=1000,\n",
    "    logging_steps=5,\n",
    "    save_steps=1000,\n",
    "    eval_steps=5,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_dir=RESULTS_DIR + \"/logs\",  # Replace with your desired logging directory\n",
    "    # report_to=\"wandb\",\n",
    "    run_name = RESULTS_DIR.split('/')[1],\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess_function(sample: Dataset, padding: str = \"max_length\") -> dict:\n",
    "    \"\"\" Preprocess the dataset. \"\"\"\n",
    "\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [item for item in sample[\"text\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample[\"label\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def postprocess_text(\n",
    "    preds: List[str], labels: List[str]\n",
    ") -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"helper function to postprocess text\"\"\"\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, average=\"macro\"\n",
    "    )\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "    ]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result\n",
    "\n",
    "\n",
    "def train() -> None:\n",
    "    \"\"\" Train the model. \"\"\"\n",
    "\n",
    "    tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=['text', 'label'])\n",
    "    print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n",
    "    \n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer,\n",
    "        mlm=False,\n",
    "    )\n",
    "\n",
    "    # Create Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"validation\"],\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # TRAIN\n",
    "    trainer.train()\n",
    "\n",
    "    # trainer.save_model(RESULTS_DIR)\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bde81a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 923/923 [00:06<00:00, 143.68 examples/s]\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tiago\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "c:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\tiago\\OneDrive - Instituto Superior de Engenharia do Porto\\Documents\\GECAD\\DiverseVul LLMs Analysis\\DiverseVul Code\\Mamba\\wandb\\run-20240320_125658-82dlp9d0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/1180939/huggingface/runs/82dlp9d0' target=\"_blank\">Mamba_Minified_CVE_Fixes_5050_batch_16</a></strong> to <a href='https://wandb.ai/1180939/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/1180939/huggingface' target=\"_blank\">https://wandb.ai/1180939/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/1180939/huggingface/runs/82dlp9d0' target=\"_blank\">https://wandb.ai/1180939/huggingface/runs/82dlp9d0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4040 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 5.43 GiB. GPU 0 has a total capacity of 16.00 GiB of which 0 bytes is free. Of the allocated memory 45.81 GiB is allocated by PyTorch, and 48.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 135\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    125\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m    126\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    127\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    131\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m    132\u001b[0m )\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# TRAIN\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# trainer.save_model(RESULTS_DIR)\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trainer\n",
      "File \u001b[1;32mc:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\transformers\\trainer.py:1779\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1777\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1778\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1779\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1780\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1784\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\transformers\\trainer.py:2117\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2116\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2117\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2120\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2121\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2123\u001b[0m ):\n\u001b[0;32m   2124\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2125\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\transformers\\trainer.py:3031\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   3028\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3030\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3031\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3033\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3034\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\transformers\\trainer.py:3054\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   3052\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3053\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 3054\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   3055\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3056\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\transformers\\models\\mamba\\modeling_mamba.py:650\u001b[0m, in \u001b[0;36mMambaForCausalLM.forward\u001b[1;34m(self, input_ids, inputs_embeds, cache_params, labels, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    643\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m    644\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m    645\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    648\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m--> 650\u001b[0m mamba_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    657\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m mamba_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    659\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype))\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[1;32mc:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\transformers\\models\\mamba\\modeling_mamba.py:553\u001b[0m, in \u001b[0;36mMambaModel.forward\u001b[1;34m(self, input_ids, inputs_embeds, cache_params, use_cache, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(mixer_block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, hidden_states, cache_params)\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 553\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mmixer_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m    556\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n",
      "File \u001b[1;32mc:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\transformers\\models\\mamba\\modeling_mamba.py:327\u001b[0m, in \u001b[0;36mMambaBlock.forward\u001b[1;34m(self, hidden_states, cache_params)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_in_fp32:\n\u001b[0;32m    325\u001b[0m     residual \u001b[38;5;241m=\u001b[39m residual\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m--> 327\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmixer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\transformers\\models\\mamba\\modeling_mamba.py:274\u001b[0m, in \u001b[0;36mMambaMixer.forward\u001b[1;34m(self, hidden_states, cache_params)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_fast_path_available \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_proj\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype:\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcuda_kernels_forward(hidden_states, cache_params)\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslow_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\transformers\\models\\mamba\\modeling_mamba.py:244\u001b[0m, in \u001b[0;36mMambaMixer.slow_forward\u001b[1;34m(self, input_states, cache_params)\u001b[0m\n\u001b[0;32m    240\u001b[0m ssm_parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_proj(hidden_states\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m    241\u001b[0m time_step, B, C \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msplit(\n\u001b[0;32m    242\u001b[0m     ssm_parameters, [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step_rank, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssm_state_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssm_state_size], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    243\u001b[0m )\n\u001b[1;32m--> 244\u001b[0m discrete_time_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[43m)\u001b[49m                                    \u001b[38;5;66;03m# [batch, seq_len, intermediate_size]\u001b[39;00m\n\u001b[0;32m    245\u001b[0m discrete_time_step \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftplus(discrete_time_step)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# [batch, intermediate_size, seq_len]\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;66;03m# 3.b. Discretization: B and C to [batch, seq_len, intermediate_size, ssm_state_size] (SRAM)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tiago\\anaconda3\\envs\\Mamba_Env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.43 GiB. GPU 0 has a total capacity of 16.00 GiB of which 0 bytes is free. Of the allocated memory 45.81 GiB is allocated by PyTorch, and 48.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8c3553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, concatenate_datasets\n",
    "\n",
    "\n",
    "# The maximum total input sequence length after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"], dataset[\"validation\"]]).map(\n",
    "    lambda x: tokenizer(x[\"text\"], truncation=True), batched=True, remove_columns=['text', 'label']\n",
    ")\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# Calculate the average input length\n",
    "average_source_length = sum(len(x) for x in tokenized_inputs[\"input_ids\"]) / len(tokenized_inputs[\"input_ids\"])\n",
    "print(f\"Average source length: {average_source_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af84d45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from calendar import IllegalWeekdayError\n",
    "import re\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=max_source_length)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5c2bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a9759c",
   "metadata": {},
   "source": [
    "# Configure LoRA (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b89236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "lora_config = LoraConfig(inference_mode=False, r=8,\n",
    "lora_alpha=32,\n",
    "lora_dropout=0.05,\n",
    "task_type=TaskType.SEQ_CLS)\n",
    "\n",
    "from peft import get_peft_model\n",
    "model_peft = get_peft_model(model, lora_config)\n",
    "model_peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b7760a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "import evaluate\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./CodeBERT_Minified_Proc_Strings_CVE_Fixes_5050_no_errors_opt_lora_batch_16\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=1e-5,\n",
    "    num_train_epochs=10,\n",
    "    logging_steps=200,\n",
    "    save_steps=200,\n",
    "    warmup_steps=1000,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    logging_dir=RESULTS_DIR + \"/logs\",  # Replace with your desired logging directory\n",
    "    report_to=\"wandb\",\n",
    "    run_name = RESULTS_DIR.split('/')[1],\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Define a Trainer instance with the prepared datasets\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "print(\"### STARTING TRAINING ###\")\n",
    "train_output = trainer.train()\n",
    "\n",
    "trainer.save_model(RESULTS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1368dfbb",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a17c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from scipy import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def evaluate(model) -> None:\n",
    "    \"\"\"Evaluate the model on the test dataset.\"\"\"\n",
    "    predictions_list, labels_list = [], []\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    test_results = model.predict(tokenized_datasets[\"test\"])\n",
    "    predictions_list = test_results.predictions.argmax(axis=1)\n",
    "\n",
    "    labels_list = tokenized_datasets[\"test\"][\"label\"]\n",
    "\n",
    "    report = classification_report(labels_list, predictions_list)\n",
    "    accuracy = accuracy_score(labels_list, predictions_list)\n",
    "    print(report)\n",
    "    print(accuracy)\n",
    "\n",
    "evaluate(trainer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e38bdacfe1d6e2b7e20a64d2eef782d7715e4314dc541be72f500d3c69b94a4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
